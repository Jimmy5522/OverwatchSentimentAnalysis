{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EloOUZclGdw7"
      },
      "source": [
        "# Procedure\n",
        "1. Data gathering\n",
        "2. Data assesment and cleaning\n",
        "3. Data Preprocessing\n",
        "4. Sentiment Analysis\n",
        "5. Data Visualization\n",
        "6. Communications and Insight\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data gathering:\n",
        "\n",
        "scrape tweets using tweepy and twitters AI\n",
        "\n",
        "collect:\n",
        "user id\n",
        "tweet content\n",
        "location\n",
        "likes\n",
        "retweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N9mxBe6_KW4u",
        "outputId": "34eabb9b-ae17-4228-be1f-f583d5167af7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tweepy\n",
        "import time\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tweepy.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZCFzm14KbHk"
      },
      "outputs": [],
      "source": [
        "#Setting up authorization to tweepy API\n",
        "#Need to fill in with correct keys. Requires an account with elevated access\n",
        "app_api_key = '*'\n",
        "app_api_key_secret = '*'\n",
        "access_token = '*'\n",
        "access_token_secret = '*'\n",
        "bearer_token = '*'\n",
        "\n",
        "\n",
        "auth = tweepy.OAuthHandler(app_api_key, app_api_key_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "\n",
        "api = tweepy.API(auth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evXlUDB-LtR7"
      },
      "outputs": [],
      "source": [
        "def scrape(search_words, date_since, num_tweets):\n",
        "  \n",
        "  #Declare Dataframe\n",
        "  df = pd.DataFrame(columns = ['username','location', 'time', 'followers', 'text', 'retweets', 'hashtags'])\n",
        "\n",
        "  #Scrape twitter for tweets\n",
        "  tweets = tweepy.Cursor(api.search_tweets, q=search_words, lang=\"en\", since_id=date_since, tweet_mode='extended').items(num_tweets)\n",
        "\n",
        "  #Store the tweets in a python list\n",
        "  tweet_list = [tweet for tweet in tweets]\n",
        "\n",
        "  #Extract information and store in dataframe\n",
        "  for tweet in tweet_list:\n",
        "    username = tweet.user.screen_name\n",
        "    location = tweet.user.location\n",
        "    time = tweet.created_at\n",
        "    followers = tweet.user.followers_count\n",
        "    retweets = tweet.retweet_count\n",
        "    hashtags = tweet.entities['hashtags']\n",
        "\n",
        "    #Change hashtags into strings\n",
        "    hashtext = ''\n",
        "    for j in range(0, len(hashtags)):\n",
        "      hashtext += ' ' + hashtags[j]['text']\n",
        "\n",
        "    #Check if it's a retweet before getting the text body\n",
        "    try:\n",
        "      text = tweet.retweeted_status.full_text\n",
        "    except AttributeError:\n",
        "      text = tweet.full_text\n",
        "\n",
        "    #Add to dataframe\n",
        "    ith_tweet = [username, location, time, followers, text, retweets, hashtext]\n",
        "    df.loc[len(df)] = ith_tweet\n",
        "  \n",
        "  return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "oosawNFFnGAG",
        "outputId": "f9e4b8a4-b88f-4caa-ab41-eb6d902845b7"
      },
      "outputs": [],
      "source": [
        "date_since = \"2022-10-04\"\n",
        "num_tweets = 500\n",
        "sleep_timer = 240 #4 minute sleep timer after each set of tweet requests, necessary to avoid overloading tweepy and getting kicked out\n",
        "\n",
        "#General consensus of game\n",
        "df_overwatch = pd.DataFrame(columns = ['username','location', 'time', 'followers', 'text', 'retweets', 'hashtags'])\n",
        "\n",
        "\n",
        "df_overwatch = pd.concat([df_overwatch, scrape(\"#Overwatch OR #Overwatch2 OR #PlayOverwatch\", date_since, num_tweets)], ignore_index=True)\n",
        "time.sleep(sleep_timer) \n",
        "\n",
        "\n",
        "#Making a dictionay of dataframes to holda data for each character\n",
        "char_dict = {}\n",
        "\n",
        "char_lst = ['Ana', 'Ashe', 'Baptiste', 'Bastion', 'Brigitte', 'Cassidy', 'D.Va', 'Doomfist', 'Echo',\n",
        "              'Genji', 'Hanzo', 'Junker_Queen', 'Junkrat', 'Lucio', 'Mei', 'Mercy', 'Moira', 'Orisa',\n",
        "              'Pharah', 'Rammatra', 'Reaper', 'Reinhardt', 'Roadhog', 'Sigma', 'Sojurn', 'Soldier_76', \n",
        "              'Sombra', 'Symmetra', 'Torbjorn', 'Tracer', 'Widowmaker', 'Winston', 'Wrecking_Ball',\n",
        "              'Zarya', 'Zenyatta']\n",
        "for i in char_lst:\n",
        "  char_dict[i] = scrape(i + \" AND Overwatch\", date_since, num_tweets)\n",
        "  time.sleep(sleep_timer) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Assessment and Cleaning:\n",
        "\n",
        "1. Remove hashtags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiRoWdBmjclH"
      },
      "outputs": [],
      "source": [
        "#Remove hashtags from text\n",
        "df_overwatch['text'] = df_overwatch['text'].str.replace('#', '', regex=True)\n",
        "\n",
        "for key in char_dict:\n",
        "  char_dict[key]['text'] = char_dict[key]['text'].str.replace('#', '', regex=True)\n",
        "  print(char_dict[key]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data preprocessing:\n",
        "\n",
        "1. Tokenize\n",
        "2. Remove stop words\n",
        "3. Lemmatization\n",
        "4. Remove anything that isn't a noun, adjective, or verb (NAV)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfVBASsilu4X"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsSxF5lHjWLm"
      },
      "outputs": [],
      "source": [
        "def make_nav(df):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  Lemmas = []\n",
        "  df['Nav'] = ''\n",
        "  lemma_text_token = []\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    text_tokens = nltk.word_tokenize(df.at[i, 'text'])\n",
        "    lemmatized_description = ' '.join([lemmatizer.lemmatize(w) for w in text_tokens])\n",
        "\n",
        "    #Lemmatize\n",
        "    lemma_text_token = nltk.word_tokenize(lemmatized_description)\n",
        "\n",
        "\n",
        "    ans = nltk.pos_tag(lemma_text_token)\n",
        "    print(ans)\n",
        "    temp = ''\n",
        "\n",
        "    #Removing stop words: only nouns, adjectives, pronouns, adverbs remain\n",
        "    for j in range(len(ans)):\n",
        "      val = ans[j][1]\n",
        "\n",
        "      if(val == 'NN' or val == 'NNS' or val == 'NNP' or val == 'NNPS' #Nouns\n",
        "        or val == 'JJ' or val == 'JJR' or val =='JJS' #Adjectives\n",
        "        or val == 'PRP' or val =='PRP$' #Pronouns\n",
        "        or val == 'RB' or val == 'RBR' or val == 'RBS' #Adverbs\n",
        "        or val == 'VB' or val == 'VBG' or val =='VBD' or val == 'VBN'):\n",
        "        temp += ans[j][0] + ' '\n",
        "      df['Nav'][i] = temp\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGG6FQAkmqEd"
      },
      "outputs": [],
      "source": [
        "#Get rid of anything that isn't a noun, adjective, or verb\n",
        "make_nav(df_overwatch)\n",
        "for key in char_dict:\n",
        "  make_nav(char_dict[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "More data cleaning:\n",
        "\n",
        "1. Make lowercase\n",
        "2. Remove extra symbols\n",
        "3. Replace dashes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvTTwiJR4_xz"
      },
      "outputs": [],
      "source": [
        "#Make lowercase\n",
        "df_overwatch['Nav'] = df_overwatch['Nav'].str.lower()\n",
        "\n",
        "#Remove urls, @ symbols, contraction tails\n",
        "df_overwatch['Nav'] = df_overwatch['Nav'].str.replace('//\\S+|http|@|\\'S+', '')\n",
        "\n",
        "#Replace dashes and underscores with spaces\n",
        "df_overwatch['Nav'] = df_overwatch['Nav'].str.replace('-|_', ' ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMVO_Q-3sDTn"
      },
      "outputs": [],
      "source": [
        "for key in char_dict:\n",
        "  char_dict[key]['Nav'] = char_dict[key]['Nav'].str.lower() #Lowercase\n",
        "  char_dict[key]['Nav'] = char_dict[key]['Nav'].str.replace('//\\S+|http|@|\\'S+', '') #remove URLS, @, contractions\n",
        "  char_dict[key]['Nav'] = char_dict[key]['Nav'].str.replace('-|_', ' ') #replace -, _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sentiment analysis/opinion mining:\n",
        "\n",
        "1. Use textBlob to get sentiment analysis\n",
        "2. Check to see overall game sentiment\n",
        "3. Create scale on most liked vs least liked character"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhWjaNdfsjE9"
      },
      "outputs": [],
      "source": [
        "#Returns the average polarity and subjectivity of a dataframe column\n",
        "#Polarity range: -1 to 1\n",
        "#Subjectivity range: 0 to 1\n",
        "def get_avg_polar_and_subjec(df_column):\n",
        "  polarity_total = 0\n",
        "  subjectivity_total = 0\n",
        "\n",
        "  for text in df_column:\n",
        "    data = TextBlob(text) #tuple with polarity in [0] and subjectivity in [1]\n",
        "    polarity_total += data.sentiment[0]\n",
        "    subjectivity_total += data.sentiment[1]\n",
        "  \n",
        "  polarity_average = polarity_total / len(df_column)\n",
        "  subjectivity_average = subjectivity_total / len(df_column)\n",
        "\n",
        "  return polarity_average, subjectivity_average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyE01DxVDKez"
      },
      "outputs": [],
      "source": [
        "game_polarity_average, game_subjectivity_average = get_avg_polar_and_subjec(df_overwatch['Nav'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZh0TXHJxygW"
      },
      "outputs": [],
      "source": [
        "#Creating a dataframe to hold subjectivity and polarity\n",
        "df_pol_sub = pd.DataFrame(columns = ['name','polarity', 'subjectivity'])\n",
        "\n",
        "for key in char_dict:\n",
        "  temp_pol_avg, temp_sub_avg = get_avg_polar_and_subjec(char_dict[key]['Nav'])\n",
        "  df_pol_sub.loc[len(df_pol_sub)] = [key, temp_pol_avg, temp_sub_avg]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Overall game stats\n",
        "print('Overall Game Polarity: ' + str(game_polarity_average))\n",
        "print('Overall Game Subjectivity: ' + str(game_subjectivity_average))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1jVBvqZ1zQL"
      },
      "outputs": [],
      "source": [
        "#Bar graph shows how much each character is liked\n",
        "labels = df_pol_sub['name']\n",
        "values = df_pol_sub['polarity']\n",
        "\n",
        "plt.figure(figsize=(38, 10))\n",
        "\n",
        "plt.bar(labels, values)\n",
        "\n",
        "plt.xlabel('Name')\n",
        "plt.ylabel('Polarity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Top 5 most liked characters\n",
        "print(\"Top 5 Most Liked Characters:\")\n",
        "liked_char = df_pol_sub.sort_values(by=['polarity']).tail(5)['name']\n",
        "liked_char = liked_char.iloc[::-1]\n",
        "print(liked_char.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Top 5 least liked characters\n",
        "print(\"Top 5 Least Liked Characters:\")\n",
        "liked_char = df_pol_sub.sort_values(by=['polarity']).head(5)['name']\n",
        "print(liked_char.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9tpD0ty4BNR"
      },
      "outputs": [],
      "source": [
        "#Bar graph shows how subjective the opinion on each character is\n",
        "labels = df_pol_sub['name']\n",
        "values = df_pol_sub['subjectivity']\n",
        "\n",
        "plt.figure(figsize=(38, 10))\n",
        "\n",
        "plt.bar(labels, values)\n",
        "\n",
        "plt.xlabel('Name')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycdHOUDqlBp3"
      },
      "outputs": [],
      "source": [
        "#Relationship between polarity and subjectivity\n",
        "x = df_pol_sub['polarity']\n",
        "y = df_pol_sub['subjectivity']\n",
        "\n",
        "plt.scatter(x, y)\n",
        "\n",
        "plt.xlabel('Polarity')\n",
        "plt.ylabel('Subjectivity')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "beedbe2faf2f7048d727558d0bc3221e7eba2a0b921cac4d4771b2feb8f74b30"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
